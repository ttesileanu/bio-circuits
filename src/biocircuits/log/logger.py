import torch
import numpy as np

from typing import Union, Iterable


class Logger:
    """Logger for tensor/array and list-of-tensor/array values.

    You can access members of the history either through `self.history`, or by using
    square brackets on `self` itself, e.g.,
        logger["foo"]

    Note that history values are undefined before a call to `self.finalize()`.

    Attributes:
    :param history: namespace holding reported values
    :param finalized: indicator whether `self.finalize()` was called
    """

    def __init__(self):
        self.finalized = False
        self.history = {}
        self._accumulator = {}

    def finalize(self):
        """Finalize recording, coalescing history into coherent arrays."""
        for key in self.history:
            self.history[key] = np.concatenate(self.history[key])

        self.finalized = True

    def log(
        self,
        key: Union[str, dict],
        value: Union[None, int, float, Iterable, torch.Tensor, np.ndarray] = None,
    ) -> "Logger":
        """Log a value or set of values.

        If `value` is a `np.ndarray`, a copy is added as-is; if it is a `torch.tensor`,
        it is converted to Numpy, then added. If it is an iterable other than tensor or
        array, it's considered a "layered" variable. An entry is recorded for each of
        its elements, with field name generated by adding `":{layer}"` to `"field"`.
        Note that therefore there is a big difference in behavior between reporting the
        2d tensor
            torch.FloatTensor([[1.0, 2.0, 3.0]])
        and reporting the list with one 1d tensor entry
            [torch.FloatTensor([1.0, 2.0, 3.0])] .

        Multiple values can be recorded at once by using a `dict` as first argument:
            logger.log({"foo": 2, "bar": 3})

        :param key: name assigned to stored value, or dictionary `{name: value}`
        :param value: value to store; needed unless `key` is a `dict`
        :return: `self`
        """
        _append_values(self.history, key, value)
        return self

    def log_batch(
        self,
        key: Union[str, dict],
        values: Union[None, Iterable, torch.Tensor, np.ndarray] = None,
    ) -> "Logger":
        """Log a batch of values.

        This is equivalent to submitting several entries. If `value` is a tensor or
        array, a new entry is generated for each row in `value`. If it is a different
        iterable, then the same is done *per layer*. This will generate as many entries
        as the length of the tensors/arrays.

        :param key: name assigned to stored values, or dictionary `{name: values}`
        :param values: values to store; needed unless `key` is a `dict`
        :return: `self`
        """
        _append_values(self.history, key, values, unsqueeze=False)
        return self

    def accumulate(
        self,
        key: Union[str, dict],
        value: Union[None, int, float, Iterable, torch.Tensor, np.ndarray] = None,
    ) -> "Logger":
        """Accumulate values to log later.

        For instance,
            logger.accumulate(field, value1)
            ...
            logger.accumulate(field, valueN)
            logger.log_accumulated()
        The values `value1`, ..., `valueN` are averaged together and the mean is logged.

        Read out the accumulated value using `self.calculate_accumulated(field)`.

        :param key: name assigned to accumulated values, or dictionary `{name: value}`
        :param value: value to add to accumulator; needed unless `key` is a `dict`
        :return: `self`
        """
        _append_values(self._accumulator, key, value, unsqueeze=False)
        return self

    def calculate_accumulated(self, key: str) -> np.ndarray:
        """Average all accumulated values for a given key."""
        if key in self._accumulator:
            values = self._accumulator[key]
            mean_value = np.mean(values, axis=0)
        else:
            mean_value = np.nan
        return mean_value

    def log_accumulated(self) -> "Logger":
        """Average all accumulated values, log them, and clear up accumulator."""
        for key in self._accumulator:
            self.log(key, self.calculate_accumulated(key))
        self._accumulator.clear()
        return self

    def __repr__(self) -> str:
        s = f"Logger(finalized={self.finalized}, "
        s += ", ".join(self.history.keys())
        s += ")"

        return s

    def __getitem__(self, key: str):
        return self.history[key]


def _append_values(
    target: dict,
    key: Union[str, dict],
    value: Union[None, int, float, Iterable, torch.Tensor, np.ndarray] = None,
    unsqueeze: bool = True,
):
    """Append values to a log dict, handling lists or tensors, as well as dicts for
    multi-parameter reports.

    If `unsqueeze` is True, a zeroth dimension is added to the value before appending.
    """
    # handle dict entry
    if not isinstance(key, str):
        if value is not None:
            raise ValueError("Logger: log used with both dict and value")

        for crt_key, crt_value in key.items():
            _append_values(target, crt_key, crt_value, unsqueeze=unsqueeze)
        return

    if not torch.is_tensor(value):
        if isinstance(value, np.ndarray):
            value = np.copy(value)
        elif hasattr(value, "__iter__"):
            for i, sub_value in enumerate(value):
                _append_values(target, f"{key}:{i}", sub_value, unsqueeze=unsqueeze)
            return
        else:
            value = np.array(value)
    else:
        value = value.detach().cpu().clone().numpy()

    if unsqueeze:
        value = np.expand_dims(value, axis=0)

    if key not in target:
        target[key] = []
    target[key].append(value)
